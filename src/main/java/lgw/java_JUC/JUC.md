**周阳老师视频笔记一（JUC）**
1) **volatile**

    volatile 是 jvm 提供的轻量级同步机制。
    
    1) 保证可见性。
        
        当多个线程访问并修改共享变量时，会将共享变量的值复制到线程各自的工作空间，当一个线程修改了这个值并把它写回到主内存之后其他的线程并不知道这个值已经发生改变。可见性就是保证该变量的修改对其他线程可见，就是主内存的值更改后会通知其他线程刷新工作空间存储的值。
    
    2) 不保证原子性。
    
        volatile 修饰的变量并不能保证修改操作是原子的，在并发修改时仍然可能出现问题。
    
    3) 禁止指令重排序。
    
        指令重排序就是虚拟机对程序的执行做的优化，将没有数据依赖关系的指令重新排序执行，提高效率。重排序时只保证单线程运行结果不变，多线程环境下执行结果是不确定的。
       
    4) 举一个使用 volatile 关键字的例子：基于 double check 的单例设计模式，demo1 演示说明。
    
2) **JMM （java内存模型）**

    它知识一种抽象的概念，并不是真是存在的，是一组规范定义了java内存的变量的访问方式。
    
    1) JMM 对于同步的规定：
    
        1) 线程解锁前必须把共享变量的值刷新回主内存。
    
        2) 线程加锁前必须读取主内存的最新值到自己的工作内存。
    
        3) 加锁解锁是同一把锁。
    
    2) JMM 的三大特性：
        
        1) 可见性。
            
            线程对变量的修改对其他线程可见，也就是修改后会通知其他线程刷新工作内存中的值。
        
        2) 原子性。
        
            指一个操作不可以被中断，操作一旦开始就不能被其他线程干扰。
                    
        3) 有序性。
        
            指令的执行并不一定是按照源码中的顺序，底层可能会进行指令重排，重排会考虑指令间的数据依赖，但只保证单线程下运行结果一致。
            
3) **CAS （Compare And Swap） AtomicXXX 类的底层实现**

    1) AtomicXXX 类
        
        1) 该类封装了对基本数据类型的操作，其中的方法都是原子性的，可以保证并发环境下的数据安全。
        
        2) 它底层并没有加锁，而是使用 CAS 实现。使用 java 中的 Unsafe 类操作底层，可以根据地址偏移量直接读取对内存中的数据。
        
    2) CAS 执行原理
        
        1) 在修改前将自己工作内存中缓存的值与主物理内存的值进行比较，如果相同则认为该值没有被改过，可以直接修改，如果不相同则本次修改失败，要先重新读取主物理内存中的值，然后重新计算结果再进行修改。
        
        2) 修改操作使用的是操作系统的底层原语实现，其执行过程是原子的，不会被打断，所以可以保证原子性。

    3) ABA 问题（demo2）
    
        1) 问题描述
        
            ABA 问题就是指，两个线程都读取了主物理内存中的变量的值 A 到自己的工作内存中，在线程一执行的过程中线程二将该变量修改为了 B ，然后又修改成了 A 。此时线程一并不知道这个值已经进行了两次修改，线程一在结束计算工作修改变量时会认为该值从来没有修改过。
        
        2) 如何解决
        
            要看具体的业务逻辑是否允许这种情况的出现，如果不允许可以使用加版本号的方式解决，规定在修改数据时必须版本号一致才能修改，否则修改失败并重做。修改时也要修改版本号。
    
    4) 高并发时多次重做问题（demo3）
    
        在高并发环境下，多个线程以 CAS 机制访问修改同一个变量。会出现某几个线程在执行任务的过程中，主物理内存中存储的值一直被别的线程修改，那么这些线程执行完任务去主物理内存修改值时会一直修改失败，然后一直重做，影响并发效率，占用 cpu 资源。对于一般的 i++ 计数操作可以使用 LongAdder 类实现，该类解决了高并发中多次重做问题。
        
    5) 只能保证一个共享变量的原子操作
    
        可以使用 AtomicReference / AtomicStampedReference 将多个共享变量封装为一个对象，使用该类将对象封装。----- AtomicReference 将实体类对象包装成可以进行原子修改的类，使用 AtomicStampedReference 包装一个可以加版本号的原子修改类。    
        
4) 各种锁

    1) 公平锁 / 非公平锁
    
        1) 公平锁 多个线程按照申请的顺序获得锁，等待时间长的会优先获得锁，吞吐量小，可以保证顺序。new ReentrantLock(true)
        
        2) 非公平锁 获得锁的顺序不确定，可能会出现有的线程长时间获取不到锁，吞吐量比较大，但是不能 保证顺序。synchronized、new ReentrantLock()
    
    2) 可重入锁（递归锁）
    
        1) 一个线程获取到锁之后，可以进入加了该锁的所有方法或者代码块。例如：A、B两个方法都用 X 对象加锁，一个线程调用A方法获取了 X 锁之后可以继续调用B方法重复获 X 取锁。
        
        2) 可重入锁可以避免死锁。
        
        3) synchronized / ReentrantLock  都是可重入锁。
    
    3) 自旋锁 (demo4 手写自旋锁)
        
        1) 概念 ---- 尝试获取锁的线程如果获取不到锁不会立即阻塞，而是采用循环的方式去尝试获取锁。
            
        2) 好处 ---- 减少了线程上下文切换的消耗。
            
        3) 缺点 ---- 一直循环会消耗 cpu资源。
            
    4) 独占锁（写锁） / 共享锁（读锁） / 互斥锁
    
        1) 独占锁，一个锁只能被一个线程持有，例如 ReentrantLock、synchronized、new ReentrantReadWriteLock.writeLock() 。
        
        2) 共享锁，一个锁可以被多个线程持有。例如 new ReentrantReadWriteLock.readLock() 
        
        3) 读与写、写与读、写与写是互斥的，读与读是不互斥的。 
        
    5) CountDownLatch（倒数闩锁） / CyclicBarrier（循环障碍） / Semaphore（信号）
    
        1) CountDownLatch （demo5）
        
            让一些线程阻塞，等待另一些线程完成后在执行。一些线程调用 await() 阻塞，其他线程调用 countDown() 倒数，计数为零时解除阻塞。解除阻塞不一定会马上执行，只是转换为 可运行 状态。
        
        2) CyclicBarrier （demo6）
        
            设置一个屏障点，当有 n 个线程在屏障点等待的时候打开屏障。与 CountDownLatch 类似，一个是减，一个是加。
        
        3) Semaphore （demo7）
        
            当 n 个线程争抢 m 个资源时（n > m）可以使用 Semaphore控制，声明一个初始值为 m 的Semaphore 抢到资源调用 acquire() 使资源数减一，释放资源调用 release() 使资源数加一。
        
            使用场景，多个共享资源的互斥使用、并发线程数的控制。
        
5) 阻塞队列（BlockingQueue）
    
    1) 概念
       
        队列为空时，取元素操作会被阻塞。队列为满时，加元素操作会被阻塞。可以使用阻塞队列实现生产者消费者模式，使用阻塞队列可以不需要手动控制 wait 和 notify 操作。
   
    2) 主要的三种阻塞队列
   
        1) ArrayBlockingQueue    由数组结构组成的有界队列。
       
        2) LinkedBlockingQueue   由链表结构组成的有界队列，默认大小为 Integer.MAX_VALUE 容量太大了，相当于无界了。。。
       
        3) SynchronousQueue（demo8）  不存储元素的阻塞队列，直接将要加入的元素交给获取元素的线程（消费者线程），如果没有消费者线程，则会阻塞。也就是每一个 put() 操作都会等待一个 take() 操作，否则阻塞。

    3) 使用场景
        
        线程池、消息中间件、生产者消费者模式（参考 ThreadDemo001 项目中两种种生产者消费者模式的实现）
        
6) 线程池 （ThreadPool）

    1) 线程池是做什么的？
        
        线程池做的工作主要是控制运行线程的数量，处理过程中将任务放入队列，然后在线程创建启动后启动这些任务，如果任务的数量超过了线程池中最大线程数量，多出来的任务排队等候，等待线程池中有线程空闲时再从队列中取出任务执行。

    2) 为什么使用线程池？使用线程池有什么优势？
    
        线程复用；控制最大并发数；管理线程；
        
        1) 降低资源消耗，通过复用已经创建的线程，减少创建线程和销毁线程带来的资源消耗。
        
        2) 提高响应速度，当任务到达时不需要等待线程创建就可以立即执行。
        
        3) 提高线程的可管理性，控制线程的数量，进行统一的分配、监控、调优。
        
    3) 怎么用（主要的三种，在ThreadDemo001项目中总结过，已经总结过的就简单过一遍，只写没总结过的）
    
        1) FixedThreadPool（LinkedBlockingQueue） 固定数量的线程池。
    
        2) SingleThreadPool（LinkedBlockingQueue） 只有一个线程的线程池。    
    
        3) CachedThreadPool（SynchronousQueue） 任务多的时候会新创建线程，每个线程都有60秒生存周期，空闲60秒自动销毁。
        
    4) 底层原理    
        
        1) 7大参数介绍
        
            1) corePoolSize 线程池中的常住核心线程数
            
            2) maximumPoolSize 能够容纳同时执行的最大线程数，此值必须大于等于1。
            
            3) keepAliveTime 多余的空闲线程存活时间。
            
            4) unit keepAliveTime 的单位。
            
            5) workQueue 任务队列，被提交但尚未执行的任务存储的队列。
            
            6) threadFactory 表示生成线程池中工作线程的线程工厂，用于创建线程，一般用默认的工厂即可。
            
            7) handler 拒绝策略，表示当前队列满了并且工作线程大于等于线程池的最大线程数（maximumPoolSize）时如何来拒绝。
        
        2) 线程池的工作原理：
        
            1) 在创建完线程池后，等待提交过来的任务请求。
            
            2) 当调用 execute() 方法添加一个任务请求时，线程池会做出如下判断：
            
                1) 如果正在运行的线程数量小于 corePoolSize，那么马上启动线程运行这个任务。
                
                2) 如果正在运行的线程数量大于等于 corePoolSize，那么将这个任务放入队列。
                
                3) 如果队列满了，且正在运行的线程数量小于 maximumPoolSize，那么就创建非核心线程完成这个任务。
                
                4) 如果队列满了，且正在运行的线程数量大于等于 maximumPoolSize，那么线程池会启动拒绝策略。
                
            3) 当一个线程完成任务是，它会从队列中取下一个任务来执行。
            
            4) 当一个线程无事可做超过一个时间（keepAliveTime）时，线程池会判断： 如果当前运行的线程数大于 corePoolSize，那么这个线程被停掉。所以线程池的所有任务完成后他最终会收缩到 corePoolSize的大小。
            
        3) 线程池的拒绝策略
        
            当等待队列满了线程池中线程数也达到了maximumPoolSize时，就需要启动拒绝策略。
            
            JDK内置的四种拒绝策略：
                
            1) AbortPolicy（默认）：直接抛出 RejectedException 阻止系统运行。
                
            2) CallerRuns： “调用者运行” 一种调节机制，该策略不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低流量。
                
            3) DiscardOldestPolicy：抛弃队列中等待最久的任务，然后把当前任务加入到队列中，尝试再次提交当前任务。
            
            4) DiscardPolicy: 直接丢弃任务，不予任何处理也不抛出异常，如果允许任务丢失，这是一种最好的方案。  
            
            